#!/usr/bin/env python3
"""
Standalone test for the new Multi-dimensional Video Evaluation System
Tests the autoraters and autoevals workflow without dependencies on existing modules.
"""

import asyncio
import os
import sys
import time
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Import our new evaluation modules directly
from visionflow.services.evaluation.video_evaluation_orchestrator import (
    VideoEvaluationOrchestrator, SamplingStrategy
)
from visionflow.services.evaluation.confidence_manager import ConfidenceManager
from visionflow.services.evaluation.score_aggregator import ScoreAggregator
from visionflow.services.evaluation.industry_metrics import IndustryMetrics

async def test_new_evaluation_system():
    """Test the new evaluation system with a generated video"""
    
    print("ğŸ¯ NEW Multi-dimensional Video Evaluation System Test")
    print("=" * 70)
    print("Testing autoraters and autoevals workflow from your slides")
    print()
    
    # Find a generated video to evaluate
    generated_dir = project_root / "generated"
    video_files = list(generated_dir.glob("*.mp4"))
    
    if not video_files:
        print("âŒ No video files found in generated/ directory")
        print("   Please generate a video first using test_hq_generation.py")
        return
    
    # Use the most recent video
    video_file = max(video_files, key=lambda p: p.stat().st_mtime)
    print(f"ğŸ“¹ Evaluating video: {video_file.name}")
    print(f"   File size: {video_file.stat().st_size / (1024*1024):.1f}MB")
    print(f"   Modified: {time.ctime(video_file.stat().st_mtime)}")
    
    # Test prompt (simulating the original generation prompt)
    test_prompt = "A beautiful sunset over a calm ocean with gentle waves, cinematic lighting, peaceful atmosphere"
    print(f"ğŸ“ Test prompt: {test_prompt}")
    print()
    
    # Initialize evaluation components
    print("ğŸš€ Initializing NEW evaluation system...")
    
    try:
        # 1. Video Evaluation Orchestrator (main coordinator)
        print("   1. Initializing Video Evaluation Orchestrator...")
        orchestrator = VideoEvaluationOrchestrator(
            sampling_strategy=SamplingStrategy.ADAPTIVE,
            max_frames_per_video=15  # Reduced for testing
        )
        
        # 2. Confidence Manager (decision making)
        print("   2. Initializing Confidence Manager...")
        confidence_manager = ConfidenceManager(
            monitoring_window_hours=1,  # Short window for testing
            low_confidence_threshold=0.3,
            high_automation_threshold=0.9
        )
        
        # 3. Score Aggregator (ensemble methods)
        print("   3. Initializing Score Aggregator...")
        score_aggregator = ScoreAggregator()
        
        # 4. Industry Metrics (objective evaluation)
        print("   4. Initializing Industry Metrics...")
        industry_metrics = IndustryMetrics()
        # Skip model initialization for now to avoid dependency issues
        # await industry_metrics.initialize_models()
        
        print("âœ… All components initialized successfully")
        print()
        
        # Run the complete evaluation workflow
        print("ğŸ” Starting 6-dimensional evaluation workflow...")
        print("=" * 50)
        print("Dimensions being evaluated:")
        print("1. ğŸ“· Visual Quality (clarity, sharpness)")
        print("2. ğŸ¨ Perceptual Quality (image similarity across frames)")
        print("3. ğŸ¬ Motion Consistency (motion smoothness, flickering)")
        print("4. ğŸ“ Text-Video Alignment (semantic and factual match)")
        print("5. âœ¨ Aesthetic Quality (factual and aesthetic consistency)")
        print("6. ğŸ“– Narrative Flow (consistent narrative flow)")
        print()
        
        start_time = time.time()
        
        # Main evaluation call
        print("ğŸ”¥ Running evaluation...")
        evaluation_result = await orchestrator.evaluate_video(
            video_path=str(video_file),
            prompt=test_prompt
        )
        
        evaluation_time = time.time() - start_time
        
        print("ğŸ¯ Processing through confidence manager...")
        # Process through confidence manager
        confidence_action = await confidence_manager.process_evaluation(evaluation_result)
        
        print("\n" + "="*70)
        print("ğŸ“Š EVALUATION RESULTS SUMMARY")
        print("="*70)
        
        # Overall Results
        print(f"ğŸ¬ Video: {video_file.name}")
        print(f"ğŸ†” Evaluation ID: {evaluation_result.evaluation_id}")
        print(f"ğŸ“ˆ Overall Score: {evaluation_result.overall_score:.3f} / 1.000")
        print(f"ğŸ¯ Overall Confidence: {evaluation_result.overall_confidence:.3f} / 1.000")
        print(f"ğŸ† Confidence Level: {evaluation_result.confidence_level.value.upper()}")
        print(f"âš–ï¸  Decision: {evaluation_result.decision.replace('_', ' ').title()}")
        print(f"ğŸ‘¥ Requires Review: {'Yes' if evaluation_result.requires_human_review else 'No'}")
        print(f"ğŸš¨ Review Priority: {evaluation_result.review_priority.upper()}")
        print()
        
        # Detailed Dimension Scores
        print("ğŸ“Š DETAILED DIMENSION SCORES")
        print("-" * 50)
        for dim_score in evaluation_result.dimension_scores:
            score_bar = "â–ˆ" * int(dim_score.score * 20) + "â–‘" * (20 - int(dim_score.score * 20))
            confidence_stars = "â­" * int(dim_score.confidence * 5)
            print(f"{dim_score.dimension.value.replace('_', ' ').title():25}: {dim_score.score:.3f} [{score_bar}] {confidence_stars}")
        print()
        
        # Evaluation Metadata
        print("âš™ï¸  EVALUATION METADATA")
        print("-" * 50)
        print(f"Sampling Strategy: {evaluation_result.sampling_strategy.value.replace('_', ' ').title()}")
        print(f"Frames Analyzed: {evaluation_result.frames_evaluated}")
        print(f"Processing Time: {evaluation_result.evaluation_time:.2f} seconds")
        print(f"Video Duration: {evaluation_result.metadata.get('video_duration', 0):.1f} seconds")
        print(f"Estimated FPS: {evaluation_result.metadata.get('fps_estimate', 0):.1f}")
        print()
        
        # Confidence Management Results
        print("ğŸ¯ CONFIDENCE MANAGEMENT ANALYSIS")
        print("-" * 50)
        print(f"Action Type: {confidence_action.action_type.value.replace('_', ' ').title()}")
        print(f"Review Priority: {confidence_action.review_priority.value.title()}")
        print(f"Confidence Score: {confidence_action.confidence_score:.3f}")
        print(f"Decision Reason: {confidence_action.reason}")
        print(f"Timestamp: {confidence_action.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        # Performance Metrics
        performance_metrics = await confidence_manager.get_performance_metrics()
        print("ğŸ“ˆ SYSTEM PERFORMANCE METRICS")
        print("-" * 50)
        print(f"Total Evaluations: {performance_metrics.total_evaluations}")
        print(f"Auto Approved: {performance_metrics.auto_approved}")
        print(f"Flagged for Review: {performance_metrics.flagged_for_review}")
        print(f"Immediate Reviews: {performance_metrics.immediate_reviews}")
        print(f"Low Confidence Rate: {performance_metrics.low_confidence_rate:.1%}")
        print(f"High Automation Rate: {performance_metrics.high_automation_rate:.1%}")
        print(f"Average Confidence: {performance_metrics.avg_confidence:.3f}")
        print()
        
        # Continuous Learning Triggers
        triggers = await confidence_manager.check_fine_tuning_triggers()
        print("ğŸ”§ CONTINUOUS LEARNING TRIGGERS")
        print("-" * 50)
        print(f"Low Confidence Trigger: {'ğŸš¨ ACTIVE' if triggers['low_confidence_trigger'] else 'âœ… OK'}")
        print(f"High Automation Trigger: {'ğŸš¨ ACTIVE' if triggers['high_automation_trigger'] else 'âœ… OK'}")
        print(f"Total Recommendations: {len(triggers['recommendations'])}")
        
        if triggers['recommendations']:
            print("\nRecommendations:")
            for i, rec in enumerate(triggers['recommendations'], 1):
                print(f"  {i}. ğŸ”„ {rec['type'].replace('_', ' ').title()} ({rec['priority']} priority)")
                print(f"     ğŸ“‹ {rec['reason']}")
                print(f"     ğŸ› ï¸  Actions: {', '.join(rec['actions'][:2])}...")
        print()
        
        # Final Assessment with Visual Indicators
        print("ğŸ‰ FINAL ASSESSMENT")
        print("=" * 50)
        
        if evaluation_result.confidence_level.value in ["excellent", "high"]:
            status_emoji = "ğŸŸ¢"
            status_text = "PASSED - High Quality Video"
            recommendation = "âœ… Approved for automatic processing"
        elif evaluation_result.confidence_level.value == "medium":
            status_emoji = "ğŸŸ¡"
            status_text = "NEEDS REVIEW - Medium Quality"
            recommendation = "âš ï¸ Queue for human review"
        else:
            status_emoji = "ğŸ”´"
            status_text = "FAILED - Low Quality"
            recommendation = "âŒ Requires immediate attention"
        
        print(f"{status_emoji} {status_text}")
        print(f"ğŸ¯ Confidence Level: {evaluation_result.overall_confidence:.1%}")
        print(f"ğŸ“‹ Decision: {evaluation_result.decision.replace('_', ' ').title()}")
        print(f"â±ï¸  Total Processing Time: {evaluation_time:.2f}s")
        print(f"ğŸ’¡ Recommendation: {recommendation}")
        
        if evaluation_result.requires_human_review:
            print(f"ğŸ‘¤ Human review queued with {evaluation_result.review_priority} priority")
        else:
            print("ğŸ¤– Fully automated processing approved")
        
        print()
        print("âœ… Multi-dimensional evaluation completed successfully!")
        print("ğŸ”„ System ready for continuous processing")
        
        return evaluation_result
        
    except Exception as e:
        print(f"âŒ Evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        return None

async def main():
    """Main test function"""
    print("ğŸ¬ Testing NEW Multi-dimensional Video Evaluation System")
    print("ğŸ“‹ Based on autoraters and autoevals principles from your slides")
    print("ğŸ”¬ Implementing 6-dimensional evaluation with confidence management")
    print()
    
    result = await test_new_evaluation_system()
    
    if result:
        print("\nğŸ’¡ IMPLEMENTATION HIGHLIGHTS:")
        print("âœ… 6-dimensional evaluation framework implemented")
        print("âœ… Confidence management with automated flagging")
        print("âœ… Score aggregation with ensemble methods")
        print("âœ… Continuous learning triggers")
        print("âœ… Frame sampling strategies for scalability")
        print("âœ… Industry metrics foundation ready")
        
        print("\nğŸš€ NEXT STEPS:")
        print("1. Integrate LPIPS, FVMD, CLIP models for objective metrics")
        print("2. Add LLaVA integration for subjective evaluation")
        print("3. Set up continuous monitoring dashboard")
        print("4. Configure human review workflows")
        print("5. Implement fine-tuning automation")
    else:
        print("\nğŸ”§ TROUBLESHOOTING:")
        print("1. Ensure video files exist in generated/ directory")
        print("2. Run video generation first: python test_hq_generation.py")
        print("3. Check system dependencies")

if __name__ == "__main__":
    asyncio.run(main())
